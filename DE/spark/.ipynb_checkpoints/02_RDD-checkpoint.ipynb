{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark 데이터 종류\n",
    "\n",
    "- RDD : Low-level API에 해당 하는 데이터, 스파크에서 동작하는 모든 데이터는 결국 RDD형태로 연산된다  \n",
    "- DataFrame : 열과 행으로 이루어진 테이블 형태의 데이터\n",
    "- DataSet : 한가지 타입의 값만 저장할 수 있는 DataFrame. PySpark에서는 사용하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD \n",
    "\n",
    "### Resillient Distributed Data\n",
    "\n",
    "Resillient : 회복력 있는  \n",
    "Distributed : 분산  \n",
    "Data : 데이터  \n",
    " * Resilient : 장애가 발생할 경우 자동으로 데이터를 복구\n",
    " * Distributed : 데이터를 읽고 쓸 때 데이터를 파티셔닝 하여 병렬로 읽고 쓴다. 직렬로 모두 읽을 때 보다 속도가 빠르다\n",
    " \n",
    " \n",
    "### RDD의 특징\n",
    "\n",
    " - Read Only : 변경불가능 객체\n",
    " - Lazy Evaluation : 늦은 수행\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 파티셔닝\n",
    "\n",
    "- 커다란 RDD를 처음 부터 끝까지 직렬로 읽고 쓰면 시간이 오래 걸린다.\n",
    "- RDD를 작은 부분(Partition) 으로 쪼개 각 Patition 을 병렬로 읽고 써 실행속도를 향상시킬 수 있다.\n",
    "- Patition을 나눠줄때는 스파크 클러스터 환경의 cpu core 숫자에 맞춰주는 것이 속도 측면에서 유리하다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rdd_partitioning](./img/rdd-partitioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### * cpu 코어가 3개일때 파티셔닝 예시\n",
    "\n",
    "![partioning](./img/partioning-optimizer.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD - 생성\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 10:57:54,590 INFO spark.SparkContext: Starting job: collect at /tmp/ipykernel_1767/603107683.py:8\n",
      "2022-09-05 10:57:54,592 INFO scheduler.DAGScheduler: Got job 2 (collect at /tmp/ipykernel_1767/603107683.py:8) with 5 output partitions\n",
      "2022-09-05 10:57:54,592 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (collect at /tmp/ipykernel_1767/603107683.py:8)\n",
      "2022-09-05 10:57:54,592 INFO scheduler.DAGScheduler: Parents of final stage: List()\n",
      "2022-09-05 10:57:54,592 INFO scheduler.DAGScheduler: Missing parents: List()\n",
      "2022-09-05 10:57:54,593 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:274), which has no missing parents\n",
      "2022-09-05 10:57:54,596 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.9 KiB, free 434.4 MiB)\n",
      "2022-09-05 10:57:54,605 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1759.0 B, free 434.4 MiB)\n",
      "2022-09-05 10:57:54,606 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:39971 (size: 1759.0 B, free: 434.4 MiB)\n",
      "2022-09-05 10:57:54,607 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1478\n",
      "2022-09-05 10:57:54,609 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ResultStage 2 (ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:274) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "2022-09-05 10:57:54,609 INFO cluster.YarnScheduler: Adding task set 2.0 with 5 tasks resource profile 0\n",
      "2022-09-05 10:57:54,611 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 7) (localhost, executor 1, partition 0, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
      "2022-09-05 10:57:54,612 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 8) (localhost, executor 2, partition 1, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
      "2022-09-05 10:57:54,630 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:43931 (size: 1759.0 B, free: 434.4 MiB)\n",
      "2022-09-05 10:57:54,631 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:40071 (size: 1759.0 B, free: 434.4 MiB)\n",
      "2022-09-05 10:57:54,641 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 9) (localhost, executor 1, partition 2, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
      "2022-09-05 10:57:54,642 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 7) in 31 ms on localhost (executor 1) (1/5)\n",
      "2022-09-05 10:57:54,643 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 10) (localhost, executor 2, partition 3, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
      "2022-09-05 10:57:54,644 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 8) in 32 ms on localhost (executor 2) (2/5)\n",
      "2022-09-05 10:57:54,652 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 11) (localhost, executor 1, partition 4, PROCESS_LOCAL, 4471 bytes) taskResourceAssignments Map()\n",
      "2022-09-05 10:57:54,653 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 9) in 12 ms on localhost (executor 1) (3/5)\n",
      "2022-09-05 10:57:54,654 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 10) in 11 ms on localhost (executor 2) (4/5)\n",
      "2022-09-05 10:57:54,661 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 11) in 9 ms on localhost (executor 1) (5/5)\n",
      "2022-09-05 10:57:54,662 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "2022-09-05 10:57:54,662 INFO scheduler.DAGScheduler: ResultStage 2 (collect at /tmp/ipykernel_1767/603107683.py:8) finished in 0.068 s\n",
      "2022-09-05 10:57:54,663 INFO scheduler.DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "2022-09-05 10:57:54,663 INFO cluster.YarnScheduler: Killing all running tasks in stage 2: Stage finished\n",
      "2022-09-05 10:57:54,663 INFO scheduler.DAGScheduler: Job 2 finished: collect at /tmp/ipykernel_1767/603107683.py:8, took 0.072727 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-05 11:09:24,210 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:39971 in memory (size: 1759.0 B, free: 434.4 MiB)\n",
      "2022-09-05 11:09:24,216 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:40071 in memory (size: 1759.0 B, free: 434.4 MiB)\n",
      "2022-09-05 11:09:24,217 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on localhost:43931 in memory (size: 1759.0 B, free: 434.4 MiB)\n",
      "2022-09-05 11:09:24,228 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:39971 in memory (size: 1758.0 B, free: 434.4 MiB)\n",
      "2022-09-05 11:09:24,231 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:40071 in memory (size: 1758.0 B, free: 434.4 MiB)\n",
      "2022-09-05 11:09:24,232 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on localhost:43931 in memory (size: 1758.0 B, free: 434.4 MiB)\n"
     ]
    }
   ],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "\n",
    "# parallelize(rdd로 만들 데이터, 파티셔닝 수)\n",
    "distData = sc.parallelize(data,5)\n",
    "\n",
    "# rdd 확인\n",
    "# collect() : rdd에 있는 데이터를 list로 반환하는 함수\n",
    "distData.collect()\n",
    "\n",
    "# 파티셔닝 개수 확인\n",
    "distData.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['하명도 스파크 50',\n",
       " '홍길동 스파크 80',\n",
       " '임꺽정 스파크 60',\n",
       " '임요환 텐서플로우 100',\n",
       " '홍진호 텐서플로우 22',\n",
       " '홍진호 텐서플로우 22',\n",
       " '이윤열 텐서플로우 90']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 5:================================================>              (17 + 2) / 22]\r\n",
      "\r\n",
      "                                                                                      \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
